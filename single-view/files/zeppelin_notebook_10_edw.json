{"paragraphs":[{"text":"%md\n## Single View - EDW Optimization\n\n#### Goal\n\nDemonstrate how [Apache Hive](http://hortonworks.com/hadoop/hive/) can help retailers get a single view of a product by combining data from different sources and run interative queries against the data.\n\nData sources for this lab are:\n  - ETL/CRM data from EDW/DB - ingested using [Apache Sqoop](http://hortonworks.com/hadoop/sqoop/)\n  - Web traffic data - ingested using [HDF](http://hortonworks.com/hdf)\n  - Social media data - ingested using [HDF](http://hortonworks.com/hdf)\n  \n#### Steps\n\n1. Use Sqoop to import CRM/ERP data from DB/EDW into Hive\n2. Use HDF to import related tweets into Hive\n3. Use HDF to import simulated web traffic logs into Hive\n4. Use Hive to analyze tables to populate statistics\n5. Use Hive to correlate the data from multiple data sources\n\nNote: this is a simplified version of the demo. A more realistic self-lab of this content is available that: \n  - runs the steps as either IT user (for setup) or marketing user (as business user running queries)\n  - uses Ranger to setup authorization policies and audit\n\nMore details at: [https://github.com/abajwa-hw/single-view-demo](https://github.com/abajwa-hw/single-view-demo)","dateUpdated":"Apr 10, 2016 1:05:43 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543594_424634311","id":"20160410-130543_1168294869","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Single View - EDW Optimization</h2>\n<h4>Goal</h4>\n<p>Demonstrate how <a href=\"http://hortonworks.com/hadoop/hive/\">Apache Hive</a> can help retailers get a single view of a product by combining data from different sources and run interative queries against the data.</p>\n<p>Data sources for this lab are:</p>\n<ul>\n<li>ETL/CRM data from EDW/DB - ingested using <a href=\"http://hortonworks.com/hadoop/sqoop/\">Apache Sqoop</a></li>\n<li>Web traffic data - ingested using <a href=\"http://hortonworks.com/hdf\">HDF</a></li>\n<li>Social media data - ingested using <a href=\"http://hortonworks.com/hdf\">HDF</a></li>\n</ul>\n<h4>Steps</h4>\n<ol>\n<li>Use Sqoop to import CRM/ERP data from DB/EDW into Hive</li>\n<li>Use HDF to import related tweets into Hive</li>\n<li>Use HDF to import simulated web traffic logs into Hive</li>\n<li>Use Hive to analyze tables to populate statistics</li>\n<li>Use Hive to correlate the data from multiple data sources</li>\n</ol>\n<p>Note: this is a simplified version of the demo. A more realistic self-lab of this content is available that:</p>\n<ul>\n<li>runs the steps as either IT user (for setup) or marketing user (as business user running queries)</li>\n<li>uses Ranger to setup authorization policies and audit</li>\n</ul>\n<p>More details at: <a href=\"https://github.com/abajwa-hw/single-view-demo\">https://github.com/abajwa-hw/single-view-demo</a></p>\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2481"},{"text":"%md\n## EDW Optimization\n\nDemonstrate how to bulk import retail data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated","dateUpdated":"Apr 10, 2016 1:05:43 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543597_421941069","id":"20160410-130543_1475564574","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>EDW Optimization</h2>\n<p>Demonstrate how to bulk import retail data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated</p>\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2482"},{"title":"Check access to PostgreSQL tables from sqoop","text":"%sh\nsqoop list-tables --connect jdbc:postgresql://localhost:5432/contoso --username zeppelin --password zeppelin -- schema contoso ","dateUpdated":"Apr 10, 2016 1:05:59 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543597_421941069","id":"20160410-130543_172124065","dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 1:05:59 PM","dateFinished":"Apr 10, 2016 1:06:01 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2483"},{"title":"Bulk load tables from PostgreSQL into Hive","text":"%sh\nsqoop import-all-tables \\\n  --username zeppelin --password zeppelin --connect jdbc:postgresql://localhost:5432/contoso \\\n  --hive-import --direct --create-hcatalog-table \\\n  --hcatalog-storage-stanza 'stored as orc TBLPROPERTIES (\"transactional\"=\"true\", \"orc.compress\"=\"SNAPPY\")' >> /tmp/sqoop.log 2>&1","dateUpdated":"Apr 10, 2016 1:11:34 PM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543598_423095316","id":"20160410-130543_1701339526","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 1:11:34 PM","dateFinished":"Apr 10, 2016 1:17:52 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2484"},{"text":"%md\n#### The above may run for a 5-10 minutes.\n\n- You can track the jobs from the YARN Resource Manager UI.\n  - *Find it from the YARN QuickLinks in Ambari*","dateUpdated":"Apr 10, 2016 1:05:43 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543598_423095316","id":"20160410-130543_2006860628","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>The above may run for a 5-10 minutes.</h4>\n<ul>\n<li>You can track the jobs from the YARN Resource Manager UI.</li>\n<li><em>Find it from the YARN QuickLinks in Ambari</em></li>\n</ul>\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2485"},{"title":"Put factsales into buckets","text":"%hive(default)\nALTER TABLE factsales CLUSTERED BY (saleskey) INTO 7 BUCKETS","dateUpdated":"Apr 10, 2016 2:12:14 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"Update Count","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"Update Count","index":0,"aggr":"sum"}}},"enabled":true,"editorMode":"ace/mode/sh","title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460295160174_-404019585","id":"20160410-133240_324726760","dateCreated":"Apr 10, 2016 1:32:40 PM","dateStarted":"Apr 10, 2016 2:12:09 PM","dateFinished":"Apr 10, 2016 2:12:10 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2486","result":{"code":"SUCCESS","type":"TABLE","msg":"Update Count\n-1\n","comment":"","msgTable":[[{"value":"-1"}]],"columnNames":[{"name":"Update Count","index":0,"aggr":"sum"}],"rows":[["-1"]]},"focus":true},{"title":"Confirm tables created in hive (text format)","text":"%hive\nshow tables","dateUpdated":"Apr 10, 2016 1:41:02 PM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tab_name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"tab_name","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543598_423095316","id":"20160410-130543_973126173","dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 1:41:02 PM","dateFinished":"Apr 10, 2016 1:41:02 PM","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2487"},{"text":"%md\n\n#### Incremental import of data into Hive from RDBMS\n\nNow that we did the one time bulk import, next we will setup an incremental sqoop job\n\nFirst we create password file containing zeppelin user's PostGres password in HDFS. This is done to allow invocations of the job to be automated/scheduled (without having to manually pass the password )","dateUpdated":"Apr 10, 2016 1:41:50 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543599_422710567","id":"20160410-130543_2489131","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Incremental import of data into Hive from RDBMS</h4>\n<p>Now that we did the one time bulk import, next we will setup an incremental sqoop job</p>\n<p>First we create password file containing zeppelin user's PostGres password in HDFS. This is done to allow invocations of the job to be automated/scheduled (without having to manually pass the password )</p>\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 1:41:50 PM","dateFinished":"Apr 10, 2016 1:41:51 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2488"},{"title":"Store PostgreSQL password (see Hadoop docs for secure method)","text":"%sh\n# use -n to ensure newline is not added\necho -n \"zeppelin\" > .password\nhadoop fs -put .password /user/zeppelin/\nrm .password","dateUpdated":"Apr 10, 2016 1:41:59 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543599_422710567","id":"20160410-130543_1061845344","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 1:41:59 PM","dateFinished":"Apr 10, 2016 1:42:01 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2489"},{"title":"Create incremental sqoop job","text":"%sh\nsqoop job -create factsales -- import --verbose \\\n  --connect 'jdbc:postgresql://localhost:5432/contoso' \\\n  --username zeppelin --password-file hdfs://0.0.0.0:8020/user/zeppelin/.password \\\n  --table factsales --check-column updatedate --incremental lastmodified --last-value '2015-01-01' \\\n  --hive-import --direct \\\n  >> /tmp/sqoop.log 2>&1\n  \n  tail /tmp/sqoop.log","dateUpdated":"Apr 10, 2016 2:11:14 PM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543599_422710567","id":"20160410-130543_693518655","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 1:46:33 PM","dateFinished":"Apr 10, 2016 1:46:35 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2490"},{"text":"%md\n#### Explanation for the incremental sqoop command above\n\nThe prior command created an incremental import sqoop job for factsales table using these switches: \n  - --table: table the job is for (i.e. factsales)\n  - --password-file: the HDFS location of the password file\n  - --incremental: lastmodified (we want to use lastmodified logic to find delta records)\n  - --check-column: specify which column that will be used to determine which delta records will be picked up (in this case, records whose updatedate column value is later than 2015-01-01 will be picked up)\n  - see [Sqoop documentation on incremental imports](https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_incremental_imports) for more details\n","dateUpdated":"Apr 10, 2016 1:05:43 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543600_433098787","id":"20160410-130543_145907041","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Explanation for the incremental sqoop command above</h4>\n<p>The prior command created an incremental import sqoop job for factsales table using these switches:</p>\n<ul>\n<li>&ndash;table: table the job is for (i.e. factsales)</li>\n<li>&ndash;password-file: the HDFS location of the password file</li>\n<li>&ndash;incremental: lastmodified (we want to use lastmodified logic to find delta records)</li>\n<li>&ndash;check-column: specify which column that will be used to determine which delta records will be picked up (in this case, records whose updatedate column value is later than 2015-01-01 will be picked up)</li>\n<li>see <a href=\"https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_incremental_imports\">Sqoop documentation on incremental imports</a> for more details</li>\n</ul>\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2491"},{"title":"Update 2 records of FactSales table (in Postgres)","text":"%sh\nexport PGPASSWORD=zeppelin\npsql -U zeppelin -d contoso -h localhost -c \"update factsales set updatedate = '2016-01-01 00:00:00' where saleskey in (3,4);\"","dateUpdated":"Apr 10, 2016 2:08:58 PM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543600_433098787","id":"20160410-130543_660669439","result":{"code":"SUCCESS","type":"TEXT","msg":"UPDATE 2\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 2:08:58 PM","dateFinished":"Apr 10, 2016 2:08:58 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2492"},{"title":"Execute incremental sqoop job","text":"%sh\nsqoop job -exec factsales >> /tmp/sqoop.log 2>&1","dateUpdated":"Apr 10, 2016 2:10:59 PM","config":{"tableHide":false,"colWidth":4,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543601_432714038","id":"20160410-130543_1346896109","result":{"code":"SUCCESS","type":"TEXT","msg":"16/04/10 14:10:28 DEBUG util.SubprocessSecurityManager: Installing subprocess security manager\n\nLogging initialized using configuration in jar:file:/usr/hdp/2.4.0.0-169/hive/lib/hive-common-1.2.1000.2.4.0.0-169.jar!/hive-log4j.properties\nOK\nTime taken: 1.195 seconds\nLoading data to table default.factsales\nchgrp: changing ownership of 'hdfs://ip-10-1-0-51.eu-west-1.compute.internal:8020/apps/hive/warehouse/factsales/part-m-00000_copy_3': User does not belong to hdfs\nTable default.factsales stats: [numFiles=4, numRows=0, totalSize=236, rawDataSize=0]\nOK\nTime taken: 0.553 seconds\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","dateStarted":"Apr 10, 2016 2:10:24 PM","dateFinished":"Apr 10, 2016 2:10:38 PM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2493"},{"text":"%md\n## Done with EDW Optimization\n\nAt this point we have shown how you can bulk import data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated","dateUpdated":"Apr 10, 2016 1:05:43 PM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543603_433483536","id":"20160410-130543_328090079","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Done with EDW Optimization</h2>\n<p>At this point we have shown how you can bulk import data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated</p>\n"},"dateCreated":"Apr 10, 2016 1:05:43 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2494"},{"dateUpdated":"Apr 10, 2016 1:05:43 PM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460293543603_433483536","id":"20160410-130543_1254092966","dateCreated":"Apr 10, 2016 1:05:43 PM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2495"}],"name":"Single View - EDW Optimization","id":"2BHEHQW1K","angularObjects":{"2BGU6YYWV":[],"2BGS4HXY1":[],"2BJE8CE8V":[],"2BJAFGWJT":[],"2BH67HDS1":[],"2BHKWKJ84":[],"2BG7MDPHE":[],"2BJNCWR5E":[],"2BGHPDVGQ":[],"2BF9863A2":[],"2BJU3MNM7":[],"2BH47F98D":[],"2BG3JJJG2":[],"2BFWAMMQB":[],"2BHSKSPHE":[]},"config":{"looknfeel":"default"},"info":{}}