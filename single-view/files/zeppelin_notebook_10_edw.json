{"paragraphs":[{"text":"%md\n## Single View - EDW Optimization\n\n#### Goal\n\nDemonstrate how [Apache Hive](http://hortonworks.com/hadoop/hive/) can help retailers get a single view of a product by combining data from different sources and run interative queries against the data.\n\nData sources for this lab are:\n  - ETL/CRM data from EDW/DB - ingested using [Apache Sqoop](http://hortonworks.com/hadoop/sqoop/)\n  - Web traffic data - ingested using [HDF](http://hortonworks.com/hdf)\n  - Social media data - ingested using [HDF](http://hortonworks.com/hdf)\n  \n#### Steps\n\n1. Use Sqoop to import CRM/ERP data from DB/EDW into Hive\n2. Use HDF to import related tweets into Hive\n3. Use HDF to import simulated web traffic logs into Hive\n4. Use Hive to analyze tables to populate statistics\n5. Use Hive to correlate the data from multiple data sources\n\nNote: this is a simplified version of the demo. A more realistic self-lab of this content is available that: \n  - runs the steps as either IT user (for setup) or marketing user (as business user running queries)\n  - uses Ranger to setup authorization policies and audit\n\nMore details at: [https://github.com/abajwa-hw/single-view-demo](https://github.com/abajwa-hw/single-view-demo)","dateUpdated":"Apr 10, 2016 11:03:21 AM","config":{"tableHide":false,"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071004_-1869891768","id":"20160410-110111_1331521199","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Single View - EDW Optimization</h2>\n<h4>Goal</h4>\n<p>Demonstrate how <a href=\"http://hortonworks.com/hadoop/hive/\">Apache Hive</a> can help retailers get a single view of a product by combining data from different sources and run interative queries against the data.</p>\n<p>Data sources for this lab are:</p>\n<ul>\n<li>ETL/CRM data from EDW/DB - ingested using <a href=\"http://hortonworks.com/hadoop/sqoop/\">Apache Sqoop</a></li>\n<li>Web traffic data - ingested using <a href=\"http://hortonworks.com/hdf\">HDF</a></li>\n<li>Social media data - ingested using <a href=\"http://hortonworks.com/hdf\">HDF</a></li>\n</ul>\n<h4>Steps</h4>\n<ol>\n<li>Use Sqoop to import CRM/ERP data from DB/EDW into Hive</li>\n<li>Use HDF to import related tweets into Hive</li>\n<li>Use HDF to import simulated web traffic logs into Hive</li>\n<li>Use Hive to analyze tables to populate statistics</li>\n<li>Use Hive to correlate the data from multiple data sources</li>\n</ol>\n<p>Note: this is a simplified version of the demo. A more realistic self-lab of this content is available that:</p>\n<ul>\n<li>runs the steps as either IT user (for setup) or marketing user (as business user running queries)</li>\n<li>uses Ranger to setup authorization policies and audit</li>\n</ul>\n<p>More details at: <a href=\"https://github.com/abajwa-hw/single-view-demo\">https://github.com/abajwa-hw/single-view-demo</a></p>\n"},"dateCreated":"Apr 10, 2016 11:01:11 AM","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3139","dateFinished":"Apr 10, 2016 11:03:20 AM","dateStarted":"Apr 10, 2016 11:03:19 AM","focus":true},{"text":"%md\n## EDW Optimization\n\nDemonstrate how to bulk import retail data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated","dateUpdated":"Apr 10, 2016 11:02:29 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071005_-1870276517","id":"20160410-110111_1196446201","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>EDW Optimization</h2>\n<p>Demonstrate how to bulk import retail data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated</p>\n"},"dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3140","focus":true},{"title":"Check access to PostgreSQL tables from sqoop","text":"%sh\nsqoop list-tables --connect jdbc:postgresql://localhost:5432/contoso --username zeppelin --password zeppelin -- schema contoso ","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071005_-1870276517","id":"20160410-110111_2087840603","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3141"},{"title":"Bulk load tables from PostgreSQL into Hive","text":"%sh\nsqoop import-all-tables --username zeppelin --password zeppelin --connect jdbc:postgresql://localhost:5432/contoso  --hive-import  --direct","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071005_-1870276517","id":"20160410-110111_580997206","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3142"},{"text":"%md\n#### The above may run for a 5-10 minutes.\n\n- You can track the jobs from the YARN Resource Manager UI.\n  - *Find it from the YARN QuickLinks in Ambari*","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071006_-1869122270","id":"20160410-110111_107819699","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>The above may run for a 5-10 minutes.</h4>\n<ul>\n<li>You can track the jobs from the YARN Resource Manager UI.</li>\n<li><em>Find it from the YARN QuickLinks in Ambari</em></li>\n</ul>\n"},"dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3143"},{"title":"Confirm Staging tables created in hive (text format)","text":"%hive\nshow tables","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"tab_name","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"tab_name","index":0,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071006_-1869122270","id":"20160410-110111_1131901982","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3144"},{"title":"Create ORC table for FactSales (command will give an empty result set)","text":"%hive\nCREATE TABLE `factsales_final` (\n`SalesKey` int ,\n`DateKey` timestamp ,  \n`channelKey` int ,  \n`StoreKey` int,\n`ProductKey` int,\n`PromotionKey` int,\n`CurrencyKey` int,\n`UnitCost` float,\n`UnitPrice` float,\n`SalesQuantity` int , \n`ReturnQuantity` int,\n`ReturnAmount` float,\n`DiscountQuantity` int,\n`DiscountAmount` float,\n`TotalCost` float,\n`SalesAmount` float,\n`ETLLoadID` int,\n`LoadDate` timestamp , \n`UpdateDate` timestamp \n )\nclustered by (saleskey) into 7 buckets\nstored as orc\nTBLPROPERTIES ('transactional'='true')","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071006_-1869122270","id":"20160410-110111_808416280","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3145"},{"title":"populate FactSales final table (command will give an empty result set)","text":"%hive\ninsert into factsales_final select * from factsales","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071006_-1869122270","id":"20160410-110111_413442475","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3146"},{"text":"%md\n\n#### Incremental import of data into Hive from RDBMS\n\nNow that we did the one time bulk import, next we will setup an incremental sqoop job\n\nFirst we create password file containing zeppelin user's PostGres password in HDFS. This is done to allow invocations of the job to be automated/scheduled (without having to manually pass the password )","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071006_-1869122270","id":"20160410-110111_1999892646","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3147"},{"title":"Store PostgreSQL password (see Hadoop docs for secure method)","text":"%sh\n# use -n to ensure newline is not added\necho -n \"zeppelin\" > .password\nhadoop fs -put .password /user/zeppelin/\nrm .password","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071006_-1869122270","id":"20160410-110111_261926362","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3148"},{"title":"Create incremental sqoop job","text":"%sh\nsqoop job -create factsales -- import --verbose --connect 'jdbc:postgresql://localhost:5432/contoso' --table factsales -username zeppelin --password-file hdfs://0.0.0.0:8020/user/zeppelin/.password --check-column updatedate --incremental lastmodified --last-value '2015-01-01' --hive-import  --direct","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"tableHide":true,"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071007_-1869507019","id":"20160410-110111_1155045253","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3149"},{"text":"%md\n#### Explanation for the incremental sqoop command above\n\nThe prior command created an incremental import sqoop job for factsales table using these switches: \n  - --table: table the job is for (i.e. factsales)\n  - --password-file: the HDFS location of the password file\n  - --incremental: lastmodified (we want to use lastmodified logic to find delta records)\n  - --check-column: specify which column that will be used to determine which delta records will be picked up (in this case, records whose updatedate column value is later than 2015-01-01 will be picked up)\n  - see [Sqoop documentation on incremental imports](https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_incremental_imports) for more details\n","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071007_-1869507019","id":"20160410-110111_2035081036","result":{"code":"SUCCESS","type":"HTML","msg":"<h4>Explanation for the incremental sqoop command above</h4>\n<p>The prior command created an incremental import sqoop job for factsales table using these switches:</p>\n<ul>\n<li>&ndash;table: table the job is for (i.e. factsales)</li>\n<li>&ndash;password-file: the HDFS location of the password file</li>\n<li>&ndash;incremental: lastmodified (we want to use lastmodified logic to find delta records)</li>\n<li>&ndash;check-column: specify which column that will be used to determine which delta records will be picked up (in this case, records whose updatedate column value is later than 2015-01-01 will be picked up)</li>\n<li>see <a href=\"https://sqoop.apache.org/docs/1.4.2/SqoopUserGuide.html#_incremental_imports\">Sqoop documentation on incremental imports</a> for more details</li>\n</ul>\n"},"dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3150"},{"title":"Update 2 records of FactSales table (in Postgres)","text":"%sh\nexport PGPASSWORD=zeppelin\npsql -U zeppelin -d contoso -h localhost -c \"update factsales set updatedate = '2016-01-01 00:00:00' where saleskey in (3,4);\"","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/sh","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071007_-1869507019","id":"20160410-110111_286438915","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3151"},{"title":"Empty the staging table in hive","text":"%hive\ntruncate table factsales","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":4,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071007_-1869507019","id":"20160410-110111_2064268166","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3152"},{"title":"Execute incremental sqoop job","text":"%sh\n\n\n\n#Copies Records Modified In PostGres To Hive\n\nsqoop job -exec factsales","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"tableHide":true,"colWidth":4,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071007_-1869507019","id":"20160410-110111_2070872029","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3153"},{"title":"Confirm only 2 records picked up","text":"%hive\nSELECT * FROM factsales limit 2","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":4,"editorMode":"ace/mode/scala","title":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"factsales.saleskey","index":0,"aggr":"sum"}],"values":[{"name":"factsales.datekey","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"factsales.saleskey","index":0,"aggr":"sum"},"yAxis":{"name":"factsales.datekey","index":1,"aggr":"sum"}}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071007_-1883742728","id":"20160410-110111_1682434371","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3154"},{"text":"%md\nIn Hive, move data from staging table to final table\n  - first **remove the common records** from final table that are also found in staging table\n  - **copy data** from staging table to final table\n  - **truncate staging table**","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071008_-1883742728","id":"20160410-110111_1617029478","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3155"},{"text":"%hive\ndelete from factsales_final where saleskey in (select saleskey from factsales)","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":4,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071008_-1883742728","id":"20160410-110111_1243424999","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3156"},{"text":"%hive\ninsert into factsales_final select * from factsales","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":4,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071008_-1883742728","id":"20160410-110111_1174513262","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3157"},{"text":"%hive\ntruncate table factsales","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":4,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071008_-1883742728","id":"20160410-110111_492257820","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3158"},{"text":"%md\nCheck the updated date on the records in final table in Hive to confirm the *updatedate* column was updated to *2016-01-01*","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071008_-1883742728","id":"20160410-110111_1922212361","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3159"},{"text":"%hive\nselect * from  factsales_final where saleskey in (1,2)","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071009_-1884127477","id":"20160410-110111_1366025900","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3160"},{"text":"%md\n## Done with EDW Optimization\n\nAt this point we have shown how you can bulk import data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated","dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071009_-1884127477","id":"20160410-110111_445782073","result":{"code":"SUCCESS","type":"HTML","msg":"<h2>Done with EDW Optimization</h2>\n<p>At this point we have shown how you can bulk import data from EDW/RDBMS into Hive and then incrementally keep the Hive tables periodically updated</p>\n"},"dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3161"},{"dateUpdated":"Apr 10, 2016 11:01:11 AM","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1460286071009_-1884127477","id":"20160410-110111_1039335125","dateCreated":"Apr 10, 2016 11:01:11 AM","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3162"}],"name":"Single View - EDW Optimization","id":"2BFBV372K","angularObjects":{"2BGWD1MCV":[],"2BG6U1JEJ":[],"2BHBXW2U5":[],"2BF8GHH7Q":[],"2BF2J222Y":[],"2BJEXH9NQ":[],"2BGYC34NJ":[],"2BHUT63RB":[],"2BGHY499A":[],"2BFYBQMRH":[],"2BFUNG2TZ":[],"2BFAAY28Y":[],"2BH5V2B5U":[],"2BJBKZYWN":[],"2BJRF82SQ":[]},"config":{"looknfeel":"default"},"info":{}}